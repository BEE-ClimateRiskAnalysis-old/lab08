{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e4e3e96d-9b80-4916-befa-54e7b348c720",
      "metadata": {},
      "source": [
        "# Lab 5 - Using `Turing.jl` for Bayesian inference\n",
        "\n",
        " **Due**: Monday, 3/20 by 1:00pm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f6bcc3",
      "metadata": {},
      "source": [
        "\n",
        "Make sure you include your name and ID below for submission. <br>\n",
        "**Name**:  <br>\n",
        "**ID**:\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5ae2990",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "### Description\n",
        "\n",
        "This lab will introduce you to the syntax and workflow for using `Turing.jl` for Bayesian inference with Julia.\n",
        "\n",
        "[`Turing.jl`](https://turinglang.org/dev/docs/using-turing/) is a probabilistic programming framework which primarily implements Hamiltonian Monte Carlo, but also allows the use of Random-Walk Metropolis-Hastings (with [`AdvancedMH.jl`](https://github.com/TuringLang/AdvancedMH.jl)) and integrates well with Julia's [Flux](https://fluxml.ai/) machine learning and differentiation packages.\n",
        "\n",
        "In this lab, you will use `Turing.jl` to fit a model for tide gauge data from [Sewell's Point, Virginia, USA](https://tidesandcurrents.noaa.gov/waterlevels.html?id=8638610) from 2015. This model is not actually a good model, as you will see, but it will introduce you to `Turing`'s syntax and the MCMC workflow.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this lab, students will be able to:\n",
        "\n",
        "- use `Turing.jl` to fit a simple statistical model using data;\n",
        "- generate and interpret diagnostic plots for Markov chain Monte Carlo convergence;\n",
        "- simulate synthetic data for posterior predictive model diagnostics."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5568b1e1",
      "metadata": {},
      "source": [
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4c4cfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load project environment\n",
        "import Pkg # load the Pkg package manager\n",
        "Pkg.activate(@__DIR__) # activate the environment in the directory of the script file\n",
        "Pkg.instantiate() # make sure all of the needed packages are installed with correct versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cb4a58",
      "metadata": {},
      "outputs": [],
      "source": [
        "using Random\n",
        "using StatsBase\n",
        "using CSV\n",
        "using Dates\n",
        "using DataFrames\n",
        "using DataFramesMeta\n",
        "using Distributions\n",
        "using Plots\n",
        "using StatsPlots\n",
        "using LaTeXStrings\n",
        "using Optim\n",
        "using Turing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9abbe706",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5a5387e1",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02508f0b-19f6-4e4d-9099-22caaf5079dd",
      "metadata": {},
      "source": [
        "As this tutorial involves random number generation, we will set a random\n",
        "seed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb147e2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Random.seed!(1);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9ff9386b-f267-4e94-a51f-a7b54c0a5702",
      "metadata": {},
      "source": [
        "> **Positive Control Tests**\n",
        ">\n",
        "> Simulating data with a known data-generating process and then trying to obtain the parameters for that process is an important step in any workflow. We have skipped that here, but a good test is to reproduce this workflow for a known linear regression model.\n",
        "\n",
        "#### Load Data\n",
        "\n",
        "Monthly-averaged tide-gauge data for Sewell's Point, VA has been provided in `data/norfolk-monthly-tide-gauge.txt` (from the [Permanent Service for Mean Sea Level](https://psmsl.org/data/obtaining/stations/299.php)). Monthly averages are convenient, because they are approximately the length of a tidal cycle, which is one source of correlation which can impact treating observations as independent. In general, tide gauge data should not be used for statistical analyses at higher frequencies unless one is accounting for the sinusoidal patterns related to tides and seasonal cycles (as NOAA does for its data products).\n",
        "\n",
        "This dataset consists of several semi-colon-delimited columns, but the first two are the ones that we care about:\n",
        "- Column 1 is the date, given in a strange decimal format: `year + (month-0.5)/12.0`. We'll want to reformat this into a more useful date-time format.\n",
        "- Column 2 is the tide gauge measurement in `mm`. The benchmark datum might be slightly different than NOAA's tide gauge record, but all that matters for our purpose is internal consistency.\n",
        "\n",
        "Let's write a function to load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cee4a03",
      "metadata": {},
      "outputs": [],
      "source": [
        "function load_data(fname)\n",
        "    date_format = DateFormat(\"yyyy-mm\")\n",
        "    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n",
        "    df = @chain fname begin\n",
        "        CSV.File(; delim=';', header=false)\n",
        "        DataFrame\n",
        "        rename(\"Column1\" => \"date\", \"Column2\" => \"gauge\")\n",
        "        # need to reformat the decimal date in the data file\n",
        "        @transform :year = Int.(floor.(:date))\n",
        "        @transform :month = Int.(round.((mod.(:date, 1) * 12) .+ 0.5))\n",
        "        @transform :datetime = Dates.format.(Date.(:year, :month), date_format)\n",
        "        # replace -99999 with missing\n",
        "        @transform :gauge = ifelse.(:gauge .== -99999, missing, :gauge)\n",
        "        select(:datetime, :gauge)\n",
        "    end\n",
        "    return df\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d532acca",
      "metadata": {},
      "outputs": [],
      "source": [
        "fname = \"data/norfolk-monthly-tide-gauge.txt\"\n",
        "tide_dat = load_data(fname)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bf0f2206",
      "metadata": {},
      "source": [
        "Next, let's plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aafcb8fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This uses the StatsPlots recipe for plotting DataFrames\n",
        "@df tide_dat plot(:datetime, :gauge; ylabel=\"Gauge Measurement (mm)\", xlabel=\"Date\", legend=:false, xrotation=30)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "82e42102",
      "metadata": {},
      "source": [
        "## Problems\n",
        "\n",
        "The goal for this lab is to quantify uncertainty in the mean rate of local sea-level rise (which includes global sea-level rise and local factors such as subsidence, which are quite important in the Norfolk area) over the 1920-2022 period at Sewell's Point. The data above looks roughly linear, so we'll use a simple linear model, assuming that the autocorrelations are independent and identically-distributed:\n",
        "$$\n",
        "\\begin{align*}\n",
        "y(t) &= \\alpha + \\beta t + \\varepsilon, \\\\\n",
        "\\varepsilon &\\sim \\text{Normal}(0, \\sigma^2).\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "702e9319",
      "metadata": {},
      "source": [
        "### Problem 1: Write Turing Model (10 points)\n",
        "\n",
        "The first task is to write a model in `Turing.jl` format. Models are specified using the `@model` macro, which acts on functions which take data (including auxiliary data, such as covariates) into account. In this case, we don't need any covariates, because our model is just based on a time index.\n",
        "\n",
        "Priors and likelihoods are specified using the following syntax: `value ~ distribution`, where the distribution uses the syntax from `Distributions.jl`. For example, to specify a $\\text{Normal}(0, 5)$ prior for a parameter $\\psi$:\n",
        "```julia\n",
        "ψ ~ Normal(0, 5)\n",
        "```\n",
        "This also makes it convenient to specify truncated distributions, using the `truncated()` function. This is handy for specifying parameters, such as variance parameters such as $\\sigma^2$, which can only take positive values.\n",
        "\n",
        "Note that the order in which the priors are specified matters, as they are sampled in sequence. So, this example (taken from the [documentation](https://turinglang.org/v0.24/docs/using-turing/guide#modelling-syntax-explained)) works:\n",
        "```julia\n",
        "s ~ Poisson(1)\n",
        "y ~ Normal(s, 1)\n",
        "```\n",
        "but this does not:\n",
        "```julia\n",
        "y ~ Normal(s, 1)\n",
        "s ~ Poisson(1)\n",
        "```\n",
        "\n",
        "For the likelihood, the syntax is similar, but sometimes there is some additional work needed to compute *e.g.* the mean of the distribution. One tip is not to create too many auxiliary variables, as this can slow the model down, or `Turing` will add additional variables. For example (and this isn't relevant here, but is on HW3), you shouldn't do the following:\n",
        "```julia\n",
        "model_out = simulation_model(...)\n",
        "residuals = y .- model_out\n",
        "for t = 1:length(residuals)\n",
        "    residuals[t] ~ Normal(0, σ²)\n",
        "end\n",
        "```\n",
        "as it will treat `residuals` as another random variable. Instead, try to specify the likelihood directly in terms of the data, as in:\n",
        "```julia\n",
        "model_out = simulation_model(...)\n",
        "for t = 1:length(y)\n",
        "    y[t] ~ Normal(model_out[t], σ²)\n",
        "end\n",
        "```\n",
        "\n",
        "There are some other tips in the [`Turing.jl` documentation](https://turinglang.org/dev/docs/using-turing/)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e1b0c45f",
      "metadata": {},
      "source": [
        "In the code block below, modify the `slr_regression` function to reflect the linear SLR model. You'll need to pick some priors: I don't need you to justify these carefully for this lab, but be thoughtful (and remember that your results can often be sensitive to these priors, so you may want to experiment a bit).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab0fb7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "@model function slr_regression(y)\n",
        "    ## specify priors; these are crazy\n",
        "    σ² ~ truncated(Normal(0, 100); lower=0)\n",
        "    a ~ Normal(0, 100)\n",
        "    b ~ Normal(0, 100)\n",
        "\n",
        "    ## specify likelihood\n",
        "    # we can specify the likelihood with a loop, calculating the relevant mean at each data point \n",
        "    # we could also rewrite this model using linear algebra to compute the joint likelihood, which is often more efficient for large and/or complex models or datasets, but the loop will be more readable in this simple case, and we won't have to worry about numerical instabilities.\n",
        "    for t = 1:length(y)\n",
        "        # this adds the (log-)likelihood of each data point to the total\n",
        "        y[t] ~ Normal(a + b * t, σ²)\n",
        "    end\n",
        "end"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "88d00e59",
      "metadata": {},
      "source": [
        "### Problem 1.2: Sample from the Posterior (5 points)\n",
        "\n",
        "Next, sample from the posterior using MCMC with the `sample` function. The default sampler used by `Turing.jl` is the [No U-Turn Sampler (NUTS)](https://arxiv.org/abs/1111.4246), which is an adaptive Hamiltonian Monte Carlo sampler. This is a good default, and there are few reasons to use a different one, unless you have an external model which can't be auto-differentiated, and therefore need to use Metropolis-Hastings (with [`AdvancedMH.jl`](https://github.com/TuringLang/AdvancedMH.jl)).\n",
        "\n",
        "The key parameters that can be set for NUTS are:\n",
        "- the number of iterations `n_adapts` used to tune the stepsize;\n",
        "- the target acceptance ratio `δ`.\n",
        "\n",
        "In this example, we will not set either, and will just use the defaults.\n",
        "\n",
        "To sample, we need to specify the following:\n",
        "- the number of iterations per chain;\n",
        "- the number of chains (for Gelman-Rubin statistics and other convergence heuristics);\n",
        "- whether to drop the warmup portion of the chain (which is a good idea, since Hamiltonian MC generally requires fewer iterations than M-H, and so these initial portions can have a stronger biasing effect).\n",
        "\n",
        "The number of chains defaults to 1, but we will usually want to run more for convergence checks. For multi-threaded or sampling, call `MCMCThreads()`, which we will do below (though this is unlikely to actually work since we didn't start this Julia instance [with multiple threads](https://docs.julialang.org/en/v1/manual/parallel-computing/#man-multithreading-1); you can change this in the settings for the Julia VS Code extension for use with notebooks, or by starting Julia with the `-t` flag at startup, *e.g.* `julia --threads 4`). You can also just remove `MCMCThreads()` from the `sample()` function call to ensure that chains are sampled in serial.\n",
        "\n",
        "It can also be convenient to include all of the sampling configuration in a `let` block, as we do below, to avoid creating additional global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6214170",
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are some missing records, so we'll drop those\n",
        "# This is ok because we're assuming all of the data are independent residuals. We could also treat them as missing data with some prior or likelihood and Turing would sample accordingly.\n",
        "dat = dropmissing(tide_dat, :gauge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78ebd39",
      "metadata": {},
      "outputs": [],
      "source": [
        "slr_model = slr_regression(dat[!, :gauge])# create the model object with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55db7c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# call the sampler with 4 chains, 5000 iterations and drop the \"burn-in/warm-up\" portion\n",
        "chain = sample(slr_model, NUTS(), MCMCThreads(), 5000, 4, drop_warmup=true);\n",
        "@show chain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "57b68bff",
      "metadata": {},
      "source": [
        "How can we interpret the output? The first parts of the summary\n",
        "statistics are straightforward: we get the mean, standard deviation, and\n",
        "Monte Carlo standard error (`mcse`) of each parameter. We also get\n",
        "information about the effective sample size (ESS) and $\\hat{R}$, the Gelman-Rubin statistic."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5ca85e71",
      "metadata": {},
      "source": [
        "### Problem 1.3: Evaluate Convergence (10 points)\n",
        "\n",
        "Plot the chains with `plot(chain)`. You can also make more specific plots with `densityplot`, `histogram`, and `traceplot` (for a full set of available plots, see [the `MCMCChains.jl` documentation](https://docs.juliahub.com/MCMCChains/QRkwo/3.0.12/)). Based on this (and any other visual diagnostics you want to make) and the quantiative diagnostics, do you think the chain has converged? Would you want to run the sampler longer?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6b043ea4",
      "metadata": {},
      "source": [
        "### Problem 1.4: Interpreting Results (10 points)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "747ff3f5",
      "metadata": {},
      "source": [
        "A useful visual representation is a corner plot (which is similar to a pairs plot). You can make a corner plot with `corner(chain)`. Make a corner plot. What can you conclude about the marginal distributions and correlations of the parameters?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3ef60c2b",
      "metadata": {},
      "source": [
        "You can also find point estimates, such as the maximum-likelihood estimate (MLE) and maximum *a posteriori* estimate (MAP) by loading `Optim.jl`. This works with the following syntax:\n",
        "```julia\n",
        "slr_model = slr_regression(data)\n",
        "optimize(slr_model, MLE()) # or MAP()\n",
        "```\n",
        "\n",
        "Find the MLE and MAP estimates. These can sometimes be useful as [starting points for the MCMC sampler](https://turinglang.org/dev/docs/using-turing/guide#sampling-with-the-mapmle-as-initial-states) as they are high-probability states. How do they compare to the full set of posterior samples? What are your thoughts on when you might want to use these estimates versus obtaining and using the full MCMC workflow?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c4e91d7d",
      "metadata": {},
      "source": [
        "### Problem 1.5: Model Diagnostics and Posterior Predictive Checks (15 points)\n",
        "\n",
        "A key part of [the Bayesian workflow](https://arxiv.org/abs/2011.01808) is to evaluate the goodness-of-fit of the model. For example, if the posteriors are too narrow, that might suggest that the prior was too restrictive (or it might not! This is just one piece of evidence). This can also guide assessments of the assumptions made when specifying the residual structure and/or the likelihood, or if there were issues with the sampling procedure.\n",
        "\n",
        "A common category of Bayesian model diagnostics are *posterior predictive checks*, which can be [visual](https://arxiv.org/abs/1709.01449) or [quantitative](http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf). These allow us to use the generative qualities of a Bayesian model to evaluate whether the posterior predictive distribution $p(\\hat{y} | y)$ adequately captures key patterns in the data. For example, we can write a function to simulate pseudo-data from a subsample of the MCMC chain. \n",
        "\n",
        "Complete the `predict_slr` function below to generate pseudo-data realizations from the MCMC samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6413bb67",
      "metadata": {},
      "outputs": [],
      "source": [
        "function predict_slr(chain)\n",
        "    # the Array(group()) syntax is more general than we need, but will work if we have multiple variables which were sampled as a group, for example multiple regression coefficients.\n",
        "    a = Array(group(chain, :a))\n",
        "    b = Array(group(chain, :b))\n",
        "    σ² = Array(group(chain, :σ²))\n",
        "\n",
        "    # write code here to simulate the model\n",
        "    y = ...\n",
        "    return y\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d817366-1205-47b2-9874-d2c8edf10629",
      "metadata": {},
      "source": [
        "Now we can generate a predictive interval and median and compare to the\n",
        "data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b705e61",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = predict_slr(chain)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "67f07743-af2d-4b39-a912-04314b3154f5",
      "metadata": {},
      "source": [
        "Notice the dimension of `y_pred`: we have 20,000 columns, because we\n",
        "have 4 chains with 5,000 samples each. If we had wanted to subsample\n",
        "(which might be necessary if we had hundreds of thousands or millions of\n",
        "samples), we could have done that within `predict_slr` before\n",
        "simulation.\n",
        "\n",
        "Next, we can get the boundaries of the 95% prediction interval for each year, along with the median. The quantiles are obtained below; plot the median and 95% prediction interval and compare to the original data (by plotting the original data on top of the prediction interval). How does it look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d954b85",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the boundaries for the 95% prediction interval and the median\n",
        "y_ci_low = quantile.(eachrow(y_pred), 0.025);\n",
        "y_ci_hi = quantile.(eachrow(y_pred), 0.975);\n",
        "y_med = quantile.(eachrow(y_pred), 0.5);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bcfe777d-826f-4d85-9572-e93d516ba8b9",
      "metadata": {},
      "source": [
        "Other visual diagnostics that we could consider include the autocorrelation and histogram of the median residuals, which could identify whether our assumption of independent and identically-distributed normal noise were reasonable. \n",
        "\n",
        "Quantitative diagnostics include the *surprise index*, which quantifies the fraction of data points which are outside of a given prediction interval. This is a valuable metric, because an ideally-calibrated model will include $\\alpha$% of the data in an $\\alpha$-prediction interval.\n",
        "\n",
        "Now, compute the surprise index for the 95% prediction interval using the quantiles computed above. What can you conclude about how well your model is calibrated? What might you change as a result?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.8.2",
      "language": "julia",
      "name": "julia-1.8"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
